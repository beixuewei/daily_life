视觉感知系统：自动驾驶最重要的感知手段

环境感知：物体检测
典型方法：滑窗法
分类器训练：n*m图像窗，正样本包含物体，负样本不包含物体；
•选择合适的阈值和窗口滑动步长；
•构建图像金字塔；
•在图像金字塔的每一层：
•对每个n*m大小的图像窗进行分类，得到响应值c；
•如果c大于阈值，相应窗口加入候选列表；
•对列表中的候选窗口按照响应值大小排序，从最高值开始，进行非最大抑制，如果有其他窗口和当前处理的候选窗口重叠超过一定比例，则从列表中移除

关键点：
•尺度处理
•非最大抑制
•图像特征
•分类器
一些经典算法：
•人脸检测：Boxfeatures（Haarwavelets）+Adaboost
•人体检测：HOG+SVM
•变形部件模型：HOG+LSVM

两阶段方法：
图像-》区域提取网络-》物体检测网络-》物体类别、位置
RCNN-》SPPNet-》FastRCNN-》FasterRCNN-》MS-CNN-》FPN-》MaskRCNN

一阶段方法：
图像-》物体检测网络-》物体类别、位置
YOLO-》SSD-》DSSD-》RRC-》Retina Net

RCNN
(2013)
SelectiveSearch生成1K~2K个候选区域
• 对每个候选区域缩放到统一尺寸，提取深度特征
• SVM分类
• 用边框回归修正结果
• 检测性能相比传统方法大幅提升，但速度太慢

SPP-NET (2014)
▪ 针对RCNN的缺点进行了改进和提升
▪ 可以接受任意输入尺度
▪ 不需要对每个候选框单独提取CNN特征
▪ 检测速度相比RCNN提升24倍，但步骤繁琐，仍然太慢！

FastRCNN
(2015)
•Fast R-CNN融合了R-CNN和SPP-Net的优点
•引入多任务损失函数，成为端到端的检测网络
•缺点就是候选框的提取使用了selective search，目标检测时间大多消耗在这上面（提候选框2~3s，而提特征分类只需0.32s）

FasterRCNN
(2015)
利用CNN网络提取候选框，真正实现了将物体检测系统框架整合到一个端到端的网络

MS-CNN
(2016)
Faster RCNN中RPN网络只在最后一个卷积层conv5_3上设置不同尺寸的anchor，不能很好的处理目标尺度变化，MS-CNN为了解决这个问题，在不同深度（不同分辨率）的特征层上设置不同尺寸的anchor，在浅层（分辨率大）设置较小anchor，高层设置较大anchor

FPN
(2017)
MS-CNN在不同层预测不同大小的目标，对于负责预测小目标的较低层，其包含的语义信息较少，而语义信息丰富的高层分辨率太低不适合预测小目标。为了增加较低层的语义信息，FPN采用Hour Glass的网络结构进行特征融合
•小目标检测的效果提升较大

YOLO
(2016)
不需要候选区域提名
▪输入图像划分为7*7网格，每个网格预测2个边框，然后对边框进行评价
▪速度快，达到50fps以上，但效果差

SSD
(2016)
借鉴Faster-RCNN中的锚点机制，在每个网格点使用3*3的滑窗提取特征，进行多个Box的回归
▪不同于Faster R-CNN，Anchor是在多个特征图上，可以利用多层特征，并自然达成多尺度检测
▪可较好地兼顾速度和精度

RetinaNet
(2017)
•一阶段检测器是基于anchor机制的dense box预测方法，因此正负样本不均衡的问题很严重，作者认为正是这个问题影响了一阶段检测器的精度。
•提出Focal Loss，利用所有anchor计算loss，根据anchor的难易(对应的confidence大小)加权其对loss的贡献, 简单的loss小, 困难的loss大。采用FPN的网络结构

关于KITTI
由德国卡尔斯鲁厄理工学院（KIT）和丰田芝加哥大学研究所（TTI）联合举办的面向自动驾驶技术现状评估的技术榜。是目前最大的、数据复杂度最高、最权威的自动驾驶感知技术测评数据库
