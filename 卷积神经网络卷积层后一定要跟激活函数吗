神经网络比线性函数更加强大的原因在于神经网络采用了非线性函数去映射数据集
可以这么说，每个神经网络（包括CNN）都使用激活函数。激活函数有很多形式，比如logistic函数，tanh函数，rectifier函数，softplus函数。在Caffe中，卷积层后跟着ReLu层，是因为convenience of the network's defination。换句话说，我们故意提取出激活函数，使得我们可以适配所有激活函数的定义，以使用适合的一个。
1）为什么使用激活函数？
没有激活函数的话，网络就与Perception（线性组合）相同了
2）为什么使用ReLu激活函数？
1>如果使用sigmoid或tanh函数，计算量很大；
2>如果使用sigmoid或tanh函数，缺点是梯度消失，导数会趋于0；
3>一些输出为0，网络变得稀疏，减少过拟合。
